# Roadmap интеграции `ProductListParser`  
*(покомпонентно‑итеративный подход)*  

> Документ описывает поэтапный план встраивания режима парсинга по списку ссылок в существующую архитектуру **Web Parser**. Базовые модули (`App.py`, `Parse.py`, `web_ui.py`) остаются «источником истины» и не ломаются — мы надстраиваемся поверх них.

---
## 1. Цель
Расширить функциональность, позволив пользователю загружать набор URL‑ов карточек товаров, автоматически собирать данные и получать сводную статистику с XLSX‑отчётом.

---
## 2. Принципы реализации
| Принцип | Что это значит на практике |
|---------|---------------------------|
| **Покомпонентность** | Меняем/добавляем один файл за раз, фиксируем; остальной код компилируется и тесты проходят. |
| **Итеративность** | Каждая итерация ≤ 1–2 раб. дня, завершается работающей «микрофичей» и ревью. |
| **Пере‑использование** | Никакой дублирующей логики: `ProductListParser` вызывает публичные методы `WebParser`. |
| **Обратная совместимость** | Режим «Стартовый парсер» работает как раньше; UI переключается вкладкой. |

---
## 3. Итерации и вехи
| # | Итерация / Веха | Основные задачи | Артефакты | Длит.* |
|---|-----------------|-----------------|-----------|--------|
| 0 | **Подготовка** | • Создать ветку `feature/product‑list`<br>• Настроить playground‑данные (3 валидные + 1 битая ссылка)<br>• Сгенерировать пустые юнит‑тесты | `tests/test_product_list_parser.py` | 0.5 д |
| 1 | **Новый класс `ProductListParser`** | • Скелет класса (init, валидация ссылок)<br>• Утилита `normalize_links()`<br>• Логирование | `product_list_parser.py` | 1 д |
| 2 | **Бизнес‑цикл** | • Метод `run()` с цикл‑обработкой ссылок (последовательный)<br>• Счётчики `total / success / failed` | Обновл. `product_list_parser.py` | 1 д |
| 3 | **Интеграция с UI** | • В `web_ui.py` добавить `st.tabs()` («Стартовый», «ProductList»)<br>• Новые поля ввода: `st.text_area` + `st.text_input` | Патч `web_ui.py` | 0.5 д |
| 4 | **Прогресс‑бар & статистика** | • Переиспользовать `_init_progress()` / `_update_progress()`<br>• Блок резюме: `st.success/fail` | Патч `web_ui.py` | 0.5 д |
| 5 | **Скачивание XLSX** | • Формирование файла в памяти (`BytesIO + pd.ExcelWriter`)<br>• `st.download_button` | XLSX‑аут, e2e‑тест | 0.5 д |
| 6 | **Тесты & CI** | • Покрытие: ≥ 80 % для нового кода<br>• GitHub Action → pytest + flake8 | `README_dev.md` + badge | 1 д |
| 7 | **Код‑ревью / Merge** | • Финальный рефакторинг (typing, docstrings)<br>• Обновить `ExistModuls.md`, добавить ссылку на `NewFeature.md` | Pull Request → `main` | 0.5 д |

\*Длительность указана ориентировочно (рабочие дни).

---

## 4. Диаграмма потоков данных (текст)
```text
UI (Streamlit, вкладка ProductList)
│ links, filename
▼
ProductListParser.run()
│ (итеративно)
▼
WebParser.get\_page() ➜ WebParser.parse\_product()
│ dict
▼
DataFrame ← агрегирует
│
▼
BytesIO (XLSX) → UI.download\_button
```

---
## 5. Риски и смягчения
| Риск | Мера |
|------|------|
| Некачественные ссылки тормозят цикл | Таймаут requests (10 с) + рetry = 3 |
| Неочевидная UX‑переключалка | Чёткие лейблы вкладок + Tooltip |
| Рост зависимостей | Используем уже установленные пакеты (`requests`, `bs4`, `pandas`, `xlsxwriter`) |
| Потеря данных при падении | Сохраняем частичные результаты каждые N ссылок (TODO v2) |

---
## 6. Definition of Done
* ✔ Все итерации смержены в `main`, CI зелёный.  
* ✔ Режим «Стартовый парсер» работает без регрессий.  
* ✔ При вводе ≥ 1 ссылки пользователь получает корректный XLSX + статистику.  
* ✔ Документация обновлена (`ExistModuls.md`, `README.md`).  
* ✔ Code Coverage ≥ 80 %, все TODO вынесены в issue‑трекер.

---
## 7. Post‑MVP бэклог
* Асинхронная обработка (`aiohttp`, семафор 10 запросов).  
* DnD‑загрузка CSV/Excel со ссылками.  
* Кэширование по URL (hashing + pickle).  
* Экспорт ошибок в отдельный XLSX «failed_links.xlsx».