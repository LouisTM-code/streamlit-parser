# –°–ø—Ä–∞–≤–æ—á–Ω–∏–∫ –º–æ–¥—É–ª–µ–π python. Web Parser
> –≠—Ç–æ—Ç –¥–æ–∫—É–º–µ–Ω—Ç —è–≤–ª—è–µ—Ç—Å—è –µ–¥–∏–Ω—ã–º –∏—Å—Ç–æ—á–Ω–∏–∫–æ–º –∏—Å—Ç–∏–Ω—ã (source of truth) –æ —Ç–µ–∫—É—â–µ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–µ –∏ —Å–æ—Å—Ç–æ—è–Ω–∏–∏ –∫–æ–¥–∞ –ø—Ä–æ–µ–∫—Ç–∞. –û–Ω —Å–ª—É–∂–∏—Ç –æ–ø–æ—Ä–æ–π –¥–ª—è AI‚Äë–≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ –∫–æ–¥–∞ –∏ –≤—Å–µ–π –∫–æ–º–∞–Ω–¥—ã —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤, –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—è –¥–µ—Ç–∞–ª—å–Ω—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º–æ–¥—É–ª–µ–π. –ò—Å–ø–æ–ª—å–∑—É—è —ç—Ç–æ—Ç —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫, AI —Å–º–æ–∂–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∏ –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å –∫–æ–¥, –æ–ø–∏—Ä–∞—è—Å—å –Ω–∞ –∞–∫—Ç—É–∞–ª—å–Ω—ã–µ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏.

---
## `App.py`
* **–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ:** –¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞ –≤ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ; –æ—Ç–≤–µ—á–∞–µ—Ç –∑–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é –æ—Å–Ω–æ–≤–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –∏ –∑–∞–ø—É—Å–∫ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞.  
* **–í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ:** –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç  
  - `WebParser`¬†‚Äî –∫–ª–∞—Å—Å –±–∏–∑–Ω–µ—Å‚Äë–ª–æ–≥–∏–∫–∏ –∏–∑ `Parse.py`.  
  - `StreamlitUI`¬†‚Äî —Å–ª–æ–π –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –∏–∑ `web_ui.py`.  
- –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –ª–∏ –∫–æ–¥ –≤–Ω—É—Ç—Ä–∏ –ø—Ä–æ—Ü–µ—Å—Å–∞¬†Streamlit (`is_streamlit_running`) –∏ –≤ –ª—é–±–æ–º —Å–ª—É—á–∞–µ —Å–æ–∑–¥–∞—ë—Ç —ç–∫–∑–µ–º–ø–ª—è—Ä `StreamlitUI`, –ø–µ—Ä–µ–¥–∞–≤–∞—è –µ–º—É –æ–±—ä–µ–∫—Ç –ø–∞—Ä—Å–µ—Ä–∞.  
**–ê–∫—Ç–∞—É–ª—å–Ω—ã–π –∫–æ–¥ `App.py`:**
```python
from Parse import WebParser
from web_ui import StreamlitUI

def main():
    parser = WebParser()
    
    is_streamlit_running()
    ui = StreamlitUI(parser)
    ui.run()

def is_streamlit_running() -> bool:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–ø—É—â–µ–Ω –ª–∏ Streamlit"""
    try:
        from streamlit.runtime.scriptrunner import get_script_run_ctx
        return get_script_run_ctx() is not None
    except ImportError:
        return False

if __name__ == "__main__":
    main()
```

---
## `Parse.py`
* **–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ:** –ò–Ω–∫–∞–ø—Å—É–ª–∏—Ä—É–µ—Ç –ª–æ–≥–∏–∫—É HTTP‚Äë–∑–∞–ø—Ä–æ—Å–æ–≤, –ø–∞—Ä—Å–∏–Ω–≥–∞ HTML‚Äë—Å—Ç—Ä–∞–Ω–∏—Ü –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –æ —Ç–æ–≤–∞—Ä–∞—Ö –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ UI –∏–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ —Ñ–∞–π–ª.
* **–í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ:** 
- –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫–∏: `requests`, `BeautifulSoup`, `pandas`, `logging`.  
- –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç –∫–ª–∞—Å—Å **`WebParser`**, –∫–æ—Ç–æ—Ä—ã–π –≤—ã–∑—ã–≤–∞–µ—Ç—Å—è –∏–∑ `App.py` –∏ `web_ui.py`.  
- –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:  
  - `BeautifulSoup`‚Äë–æ–±—ä–µ–∫—Ç—ã (–º–µ—Ç–æ–¥ `get_page`)  
  - –°–ø–∏—Å–∫–∏ —Å—Å—ã–ª–æ–∫ (`parse_links`)  
  - –°–ª–æ–≤–∞—Ä–∏ —Å –¥–∞–Ω–Ω—ã–º–∏ –æ —Ç–æ–≤–∞—Ä–µ (`parse_product`)  
  - Excel‚Äë—Ñ–∞–π–ª (–º–µ—Ç–æ–¥ `save_to_excel`).
**–ê–∫—Ç–∞—É–ª—å–Ω—ã–π –∫–æ–¥ `Parse.py`:**
```python
import requests
from bs4 import BeautifulSoup
import pandas as pd
import logging
from typing import Optional, List, Dict

class WebParser:
    def __init__(self):
        self.setup_logging()
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })

    @staticmethod
    def setup_logging():
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[logging.StreamHandler()]
        )

    @staticmethod
    def clean_text(text: str) -> str:
        return ' '.join(text.replace('\xa0', ' ').strip().split())

    def get_page(self, url: str) -> Optional[BeautifulSoup]:
        try:
            response = self.session.get(url)
            response.raise_for_status()
            response.encoding = response.apparent_encoding
            return BeautifulSoup(response.text, 'html.parser')
        except requests.exceptions.RequestException as e:
            logging.error(f'–û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ {url}: {str(e)}')
            return None

    def parse_links(self, soup: BeautifulSoup) -> List[str]:
        """–°–±–æ—Ä —Å—Å—ã–ª–æ–∫ —Å –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –¥–≤—É—Ö —Ä–∞–∑–Ω—ã—Ö —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–≤"""
        links = []
        
        # –ü–µ—Ä–≤—ã–π –≤–∞—Ä–∏–∞–Ω—Ç (–æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Å–µ–ª–µ–∫—Ç–æ—Ä)
        for link in soup.select('td.gr-title.hidden_mobile a[href]'):
            href = link.get('href', '')
            if href.startswith('http'):
                links.append(href)
                logging.debug(f'–ù–∞–π–¥–µ–Ω–∞ —Å—Å—ã–ª–∫–∞ (–≤–∞—Ä–∏–∞–Ω—Ç 1): {href}')
        
        # –í—Ç–æ—Ä–æ–π –≤–∞—Ä–∏–∞–Ω—Ç (–Ω–æ–≤—ã–π —Å–µ–ª–µ–∫—Ç–æ—Ä)
        for link in soup.select('div.name_value_pc a[href]'):
            href = link.get('href', '')
            if href.startswith('http'):
                links.append(href)
                logging.debug(f'–ù–∞–π–¥–µ–Ω–∞ —Å—Å—ã–ª–∫–∞ (–≤–∞—Ä–∏–∞–Ω—Ç 2): {href}')
        
        # –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –ø–æ—Ä—è–¥–∫–∞
        seen = set()
        return [x for x in links if not (x in seen or seen.add(x))]

    def parse_features(self, soup: BeautifulSoup) -> Dict[str, str]:
        features = {}
        try:
            for feature_div in soup.find_all('div', class_='cnc-product-features__feature'):
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
                label = feature_div.find('span', class_='cnc-product-features__label')
                if not label:
                    continue
                
                feature_name = self.clean_text(label.text).rstrip(':')
                value_div = feature_div.find('div')

                if not feature_name or not value_div:
                    continue

                # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ –∑–Ω–∞—á–µ–Ω–∏–π
                if value_div.find('a'):
                    # –°–ª—É—á–∞–π —Å–æ —Å—Å—ã–ª–∫–æ–π
                    value = self.clean_text(value_div.find('a').text)
                elif value_div.find('ul'):
                    # –°–ª—É—á–∞–π —Å–æ —Å–ø–∏—Å–∫–æ–º
                    items = [self.clean_text(li.text) for li in value_div.find_all('li')]
                    value = ', '.join(items)
                else:
                    # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Å–ª—É—á–∞–π
                    value = self.clean_text(value_div.text.strip())

                features[feature_name] = value

        except Exception as e:
            logging.error(f'–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫: {str(e)}')
        
        return features

    def parse_product(self, soup: BeautifulSoup) -> Dict[str, str]:
        product_data = {
            '–¢–æ–≤–∞—Ä': '–ù/–î',
            '–¶–µ–Ω–∞': '–ù/–î',
            '–û–ø–∏—Å–∞–Ω–∏–µ': '–ù/–î',
            '–ê—Ä—Ç–∏–∫—É–ª': '–ù/–î'
        }

        try:
            # –û—Å–Ω–æ–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            title = soup.find('h1', class_='cnc-product-detail__title')
            if title:
                product_data['–¢–æ–≤–∞—Ä'] = self.clean_text(title.text)

            price_div = soup.find('div', class_='cnc-product-detail__price-actual')
            if price_div:
                price = price_div.find('span', class_='ty-price-num')
                if price:
                    product_data['–¶–µ–Ω–∞'] = self.clean_text(price.text)

            description_div = soup.find('div', class_='cnc-product-description__left')
            if description_div:
                paragraphs = description_div.find_all(
                    'p', class_=lambda x: x != 'cnc-product-description__notice'
                )
                product_data['–û–ø–∏—Å–∞–Ω–∏–µ'] = ' '.join(
                    self.clean_text(p.text) for p in paragraphs if p.text.strip()
                )

            sku = soup.find('span', class_='g-js-text-for-copy cnc-product-detail__product-code')
            if sku:
                product_data['–ê—Ä—Ç–∏–∫—É–ª'] = self.clean_text(sku.text)

            # –î–æ–±–∞–≤–ª—è–µ–º —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
            product_data.update(self.parse_features(soup))
            
            logging.info(f'–ò–∑–≤–ª–µ—á–µ–Ω–æ {len(product_data)-4} —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫')

        except Exception as e:
            logging.error(f'–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ç–æ–≤–∞—Ä–∞: {str(e)}')

        return product_data

    @staticmethod
    def save_to_excel(data: List[Dict], filename: str) -> None:
        try:
            df = pd.DataFrame(data)
            df.to_excel(filename, index=False)
            logging.info(f'–§–∞–π–ª {filename} —Å–æ—Ö—Ä–∞–Ω—ë–Ω ({len(df.columns)} —Å—Ç–æ–ª–±—Ü–æ–≤)')
        except Exception as e:
            logging.error(f'–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {str(e)}')
```

---
## `web_ui.py`
* **–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ:** –†–µ–∞–ª–∏–∑—É–µ—Ç –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –Ω–∞¬†Streamlit –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ–º –ø–∞—Ä—Å–∏–Ω–≥–∞ –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.  
* **–í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ:** 
- –ü—Ä–∏–Ω–∏–º–∞–µ—Ç –æ–±—ä–µ–∫—Ç `WebParser`, –≤—ã–∑—ã–≤–∞—è –µ–≥–æ –º–µ—Ç–æ–¥—ã –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü –∏ –ø–∞—Ä—Å–∏–Ω–≥–∞.  
- –ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç—Å—è —Å–æ Streamlit API: —Å–∞–π–¥–±–∞—Ä, –ø—Ä–æ–≥—Ä–µ—Å—Å‚Äë–±–∞—Ä, —Å–ø–∏–Ω–Ω–µ—Ä—ã, `st.dataframe`, `st.download_button`.  
- –í—ã–≤–æ–¥–∏—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é DataFrame –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç Excel‚Äë—Ñ–∞–π–ª —á–µ—Ä–µ–∑¬†download‚Äë–∫–Ω–æ–ø–∫—É.  
**–ê–∫—Ç–∞—É–ª—å–Ω—ã–π –∫–æ–¥ `web_ui.py`:**
```python
import streamlit as st
import time
from io import BytesIO
from typing import Any, Dict, List, Optional, Tuple
import pandas as pd
from Parse import WebParser
from product_list_parser import ProductListParser

class StreamlitUI:
    def __init__(self, parser: WebParser):
        self.parser = parser
        self._setup_page_config()
        self.progress_bar = None
        self.status_text = None
        self.stats_placeholder = None
    # ------------------------------------------------------------------ #
    #                        BASIC PAGE CONFIG                           #
    # ------------------------------------------------------------------ #
    @staticmethod
    def _setup_page_config():
        st.set_page_config(
            page_title="Web Parser",
            layout="centered",
            page_icon="üîç",
            initial_sidebar_state="expanded",
        )
    # ------------------------------------------------------------------ #
    #                        SIDEBAR / TABS                              #
    # ------------------------------------------------------------------ #
    def render_sidebar(self) -> Optional[dict]:
        """–û—Ç—Ä–∏—Å–æ–≤–∫–∞ –±–æ–∫–æ–≤–æ–π –ø–∞–Ω–µ–ª–∏ —Å –¥–≤—É–º—è –≤–∫–ª–∞–¥–∫–∞–º–∏:
        1. –°—Ç–∞—Ä—Ç–æ–≤—ã–π –ø–∞—Ä—Å–µ—Ä  (—Å—Ç–∞—Ä—ã–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª)
        2. ProductListParser (–º–∞—Å—Å–æ–≤—ã–π –ø–∞—Ä—Å–∏–Ω–≥ —Å–ø–∏—Å–∫–∞ —Å—Å—ã–ª–æ–∫)
        """
        with st.sidebar:
            st.title("‚öôÔ∏è¬†–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø–∞—Ä—Å–µ—Ä–æ–º")
            tab_start, tab_list = st.tabs(["–°—Ç–∞—Ä—Ç–æ–≤—ã–π", "ProductList"])

            params: Optional[dict] = None

            # ---------- –í–∫–ª–∞–¥–∫–∞ 1 ‚Äì –°—Ç–∞—Ä—Ç–æ–≤—ã–π –ø–∞—Ä—Å–µ—Ä ------------------- #
            with tab_start:
                url = st.text_input(
                    "–°—Ç–∞—Ä—Ç–æ–≤—ã–π URL", "https://example.com", key="start_url"
                )
                output_file = st.text_input(
                    "–ò–º—è —Ñ–∞–π–ª–∞", "products.xlsx", key="start_output"
                )
                if st.button(
                    "üöÄ¬†–ù–∞—á–∞—Ç—å –ø–∞—Ä—Å–∏–Ω–≥", key="start_button", use_container_width=True
                ):
                    params = {
                        "mode": "start",
                        "url": url,
                        "output": output_file,
                    }
            # ---------- –í–∫–ª–∞–¥–∫–∞ 2 ‚Äì ProductListParser ------------------ #
            with tab_list:
                links_text = st.text_area(
                    "–°—Å—ã–ª–∫–∏ (–ø–æ –æ–¥–Ω–æ–π –Ω–∞ —Å—Ç—Ä–æ–∫–µ)",
                    height=200,
                    placeholder="https://example.com/product/123",
                    key="links_input",
                )
                output_file_links = st.text_input(
                    "–ò–º—è —Ñ–∞–π–ª–∞",
                    "product_list.xlsx",
                    key="links_output",
                )
                if st.button(
                    "üöÄ¬†–ó–∞–ø—É—Å—Ç–∏—Ç—å ProductListParser",
                    key="list_button",
                    use_container_width=True,
                ):
                    raw_links = [ln for ln in links_text.splitlines() if ln.strip()]
                    params = {
                        "mode": "productlist",
                        "links": raw_links,
                        "output": output_file_links,
                    }

            st.markdown("---")
            self.stats_placeholder = st.empty()

        return params
    # ------------------------------------------------------------------ #
    #                       COMMON PROGRESS HELPERS                      #
    # ------------------------------------------------------------------ #
    def _init_progress(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞"""
        self.progress_bar = st.progress(0)
        self.status_text = st.empty()
        self.stats_placeholder = st.empty()

    def _update_progress(self, value: float, status: str):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞"""
        self.progress_bar.progress(int(value))
        self.status_text.markdown(f"**–°—Ç–∞—Ç—É—Å:** {status}")

    def _show_stats(self, total: int, processed: int):
        """–û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        self.stats_placeholder.markdown(
            f"""
        ### üìä¬†–ü—Ä–æ–≥—Ä–µ—Å—Å
        - –í—Å–µ–≥–æ: **{total}**
        - –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: **{processed}**
        - –û—Å—Ç–∞–ª–æ—Å—å: **{total - processed}**
        """
        )
    # ------------------------------------------------------------------ #
    #                   RENDER RESULTS :  START PARSER                   #
    # ------------------------------------------------------------------ #
    def render_results(self, data: pd.DataFrame, filename: str):
        """–û—Ç—Ä–∏—Å–æ–≤–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–∞—Ä—Å–∏–Ω–≥–∞ (—Å—Ç–∞—Ä—ã–π —Ä–µ–∂–∏–º)"""
        st.success("‚úÖ¬†–ü–∞—Ä—Å–∏–Ω–≥ —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω!")

        with st.expander("üìÅ¬†–ü—Ä–æ—Å–º–æ—Ç—Ä –¥–∞–Ω–Ω—ã—Ö", expanded=True):
            st.dataframe(data, use_container_width=True, height=400)

        output = BytesIO()
        with pd.ExcelWriter(output, engine="xlsxwriter") as writer:
            data.to_excel(writer, index=False, sheet_name="Products")

        st.download_button(
            label="üíæ¬†–°–∫–∞—á–∞—Ç—å Excel",
            data=output.getvalue(),
            file_name=filename,
            mime=(
                "application/"
                "vnd.openxmlformats-officedocument.spreadsheetml.sheet"
            ),
            use_container_width=True,
        )
    # ------------------------------------------------------------------ #
    #               RENDER RESULTS :  PRODUCT LIST PARSER                #
    # ------------------------------------------------------------------ #
    def render_product_list_results(
        self,
        stats: Dict[str, Any],
        excel_content: bytes,
        filename: str,
    ):
        """–í—ã–≤–æ–¥–∏—Ç —Å–≤–æ–¥–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É + –∫–Ω–æ–ø–∫—É —Å–∫–∞—á–∏–≤–∞–Ω–∏—è Excel —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –ª–∏—Å—Ç–∞–º–∏"""
        st.success("‚úÖ¬†–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ø–∏—Å–∫–∞ —Å—Å—ã–ª–æ–∫ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
        st.subheader("üìä¬†–ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞")
        st.markdown(
            f"""
        - –í—Å–µ–≥–æ —Å—Å—ã–ª–æ–∫: **{stats['total']}**
        - –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: **{stats['success']}**
        - –û—à–∏–±–æ–∫: **{stats['failed']}**
        - –¢–æ–≤–∞—Ä–æ–≤ —Å–æ–±—Ä–∞–Ω–æ: **{stats['total_products']}**
        """
        )

        if stats["failed"]:
            with st.expander("‚ö†Ô∏è¬†–°—Å—ã–ª–∫–∏ —Å –æ—à–∏–±–∫–∞–º–∏"):
                st.write(stats["failed_links"])

        # –∫–Ω–æ–ø–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –º–Ω–æ–≥–æ-–ª–∏—Å—Ç–æ–≤–æ–≥–æ —Ñ–∞–π–ª–∞
        st.download_button(
            label="üíæ¬†–°–∫–∞—á–∞—Ç—å Excel",
            data=excel_content,
            file_name=filename,
            mime=(
                "application/"
                "vnd.openxmlformats-officedocument.spreadsheetml.sheet"
            ),
            use_container_width=True,
        )
    # ------------------------------------------------------------------ #
    #                             MAIN LOOP                              #
    # ------------------------------------------------------------------ #
    def run(self):
        st.title("üîç¬†Web Parser")

        params = self.render_sidebar()
        if not params:
            return

        self._init_progress()
        try:
            if params["mode"] == "start":
                result = self._run_parsing(params)
                if result:
                    self.render_results(*result)
            else:  # mode == productlist
                stats, excel_data, out_file = self._run_product_list(params)
                self.render_product_list_results(stats, excel_data, out_file)
        except Exception as exc:
            st.error(f"‚õî¬†–û—à–∏–±–∫–∞: {exc}")
        finally:
            time.sleep(0.5)
            self.progress_bar.empty()
            self.status_text.empty()
    # ------------------------------------------------------------------ #
    #                      ORIGINAL START‚ÄëPARSER FLOW                    #
    # ------------------------------------------------------------------ #
    def _run_parsing(self, params: dict) -> Optional[Tuple[pd.DataFrame, str]]:
        """–ü—Ä–æ—Ü–µ—Å—Å –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–ª—è —Å—Ç–∞—Ä—Ç–æ–≤–æ–≥–æ URL (–æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∂–∏–º)"""
        self._update_progress(5, "–ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç–∞—Ä—Ç–æ–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã‚Ä¶")
        start_page = self.parser.get_page(params["url"])
        if not start_page:
            raise Exception("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç–∞—Ä—Ç–æ–≤—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É")

        self._update_progress(15, "–ü–æ–∏—Å–∫ —Å—Å—ã–ª–æ–∫ –Ω–∞ —Ç–æ–≤–∞—Ä—ã‚Ä¶")
        links = self.parser.parse_links(start_page)
        if not links:
            raise Exception("–°—Å—ã–ª–∫–∏ –Ω–∞ —Ç–æ–≤–∞—Ä—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")

        total = len(links)
        products: List[Dict[str, Any]] = []

        for idx, link in enumerate(links, 1):
            try:
                progress = 15 + int(70 * (idx / total))
                self._update_progress(progress, f"–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–æ–≤–∞—Ä–∞ {idx}/{total}")
                self._show_stats(total, idx)

                with st.spinner(f"–û–±—Ä–∞–±–æ—Ç–∫–∞: {link.split('/')[-1]}"):
                    product_page = self.parser.get_page(link)
                    if product_page:
                        products.append(self.parser.parse_product(product_page))
                    time.sleep(0.1)  # –∏–º–∏—Ç–∞—Ü–∏—è –∑–∞–¥–µ—Ä–∂–∫–∏
            except Exception as ex:
                st.warning(f"–ü—Ä–æ–ø—É—â–µ–Ω —Ç–æ–≤–∞—Ä {idx}: {ex}")

        self._update_progress(95, "–§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç—á—ë—Ç–∞‚Ä¶")
        df = pd.DataFrame(products)
        if df.empty:
            raise Exception("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–±—Ä–∞—Ç—å –¥–∞–Ω–Ω—ã–µ")

        return df, params["output"]
    # ------------------------------------------------------------------ #
    #                 NEW FLOW  ‚Äì  PRODUCT LIST PARSER                   #
    # ------------------------------------------------------------------ #
    def _run_product_list(
        self, params: dict
    ) -> Tuple[Dict[str, Any], bytes, str]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω–æ–≥–æ —Å–ø–∏—Å–∫–∞ URL‚Äë–∞–¥—Ä–µ—Å–æ–≤"""
        links: List[str] = params["links"]
        total = len(links)
        if total == 0:
            raise Exception("–°–ø–∏—Å–æ–∫ —Å—Å—ã–ª–æ–∫ –ø—É—Å—Ç")

        self._update_progress(5, "–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ProductListParser‚Ä¶")
        pl_parser = ProductListParser(
            links=links, output_file=params["output"], base_parser=self.parser
        )

        failed_links: List[str] = []
        # --- –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å—Å—ã–ª–∫–∏ --------------------- #
        for idx, link in enumerate(pl_parser.links, 1):
            progress = 5 + int(85 * (idx / total))
            self._update_progress(progress, f"–û–±—Ä–∞–±–æ—Ç–∫–∞ {idx}/{total}")
            self._show_stats(total, idx)

            try:
                soup = self.parser.get_page(link)
                if not soup:
                    failed_links.append(link)
                    continue

                products_in_page = pl_parser._parse_category_page(soup)

                title = pl_parser._extract_page_title(soup)
                sheet_name = pl_parser._make_unique_sheet_name(title)
                pl_parser._sheet_data[sheet_name] = products_in_page

            except Exception as ex:
                st.warning(f"–ü—Ä–æ–ø—É—â–µ–Ω–∞ —Å—Å—ã–ª–∫–∞ {idx}: {ex}")
                failed_links.append(link)
        # --- —Ñ–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ ------------------------------------ #
        stats = {
            "total": total,
            "success": total - len(failed_links),
            "failed": len(failed_links),
            "failed_links": failed_links,
            "total_products": sum(len(v) for v in pl_parser._sheet_data.values()),
        }

        self._update_progress(95, "–§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç—á—ë—Ç–∞‚Ä¶")
        excel_bytes = pl_parser.save_results()
        return stats, excel_bytes, params["output"]
```

---
## `product_list_parser.py`
* **–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ:** –ú–æ–¥—É–ª—å —Ä–µ–∞–ª–∏–∑—É–µ—Ç –∫–ª–∞—Å—Å **ProductListParser** ‚Äì¬†–æ–±—ë—Ä—Ç–∫—É –Ω–∞–¥ `WebParser`, –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–Ω—É—é –¥–ª—è –ø–∞–∫–µ—Ç–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞ *–∫–∞—Ç–µ–≥–æ—Ä–∏–π* –∏–ª–∏ –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω–æ–≥–æ —Å–ø–∏—Å–∫–∞ URL‚Äë–∞–¥—Ä–µ—Å–æ–≤. –í¬†–¥–∞–Ω–Ω–æ–π –≤–µ—Ä—Å–∏–∏ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–∞ –ø–æ–¥–¥–µ—Ä–∂–∫–∞ –Ω–æ–≤–æ–π HTML‚Äë—Ä–∞–∑–º–µ—Ç–∫–∏ —Å–∞–π—Ç–∞
* **–í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ:** 
- –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π —Å—Å—ã–ª–æ–∫
- –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—é –≤—Ö–æ–¥–Ω—ã—Ö URL‚Äë–∞–¥—Ä–µ—Å–æ–≤
- –±–∞–∑–æ–≤—É—é –Ω–∞—Å—Ç—Ä–æ–π–∫—É –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
- –º–µ—Ç–æ–¥ run() –¥–ª—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Å—ã–ª–æ–∫
- –ö–ª–∞—Å—Å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ –æ–±—Ö–æ–¥–∏—Ç –≤—Å–µ –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–µ —Å—Å—ã–ª–∫–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏, –∏–∑–≤–ª–µ–∫–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Ç–æ–≤–∞—Ä–æ–≤ –≤¬†–∫–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏ –∞–∫–∫—É–º—É–ª–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã.
> –ü–æ–ª–Ω–∞—è –±–∏–∑–Ω–µ—Å‚Äë–ª–æ–≥–∏–∫–∞ (–∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥, —Å–±–æ—Ä —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏,
—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ XLSX) –±—É–¥–µ—Ç –¥–æ–±–∞–≤–ª–µ–Ω–∞ –Ω–∞ —Å–ª–µ–¥—É—é—â–∏—Ö –∏—Ç–µ—Ä–∞—Ü–∏—è—Ö –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å NewFeature.md

**–ê–∫—Ç–∞—É–ª—å–Ω—ã–π –∫–æ–¥ `product_list_parser.py`:**
```python
from __future__ import annotations
import logging
import re
from collections import OrderedDict
from io import BytesIO
from pathlib import Path
from typing import Any, Dict, List, Tuple
from urllib.parse import parse_qsl, urlencode, urlparse, urlunparse
import pandas as pd
from bs4 import BeautifulSoup, Tag
from Parse import WebParser

__all__ = ["ProductListParser"]
# ========================================================================= #
#                               –ö–õ–ê–°–°                                        #
# ========================================================================= #
class ProductListParser:
    # ------------------------------------------------------------------ #
    #                         –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è                               #
    # ------------------------------------------------------------------ #
    def __init__(
        self,
        links: List[str],
        output_file: str = "product_list.xlsx",
        base_parser: WebParser | None = None,
    ) -> None:
        self.logger: logging.Logger = self._configure_logger()
        self.parser: WebParser = base_parser or WebParser()
        self.output_file: str = output_file

        self.links: List[str] = self.normalize_links(links)
        self._validate_links()
        self.logger.info("–ü—Ä–∏–Ω—è—Ç–æ %d —Å—Å—ã–ª–æ–∫", len(self.links))

        # –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–ª—è —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è Excel
        self._sheet_name_counts: Dict[str, int] = {}
        self._sheet_data: "OrderedDict[str, List[Dict[str, Any]]]" = OrderedDict()
    # ------------------------------------------------------------------ #
    #                         –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ                                #
    # ------------------------------------------------------------------ #
    @staticmethod
    def _configure_logger() -> logging.Logger:
        logger = logging.getLogger("ProductListParser")
        if not logger.handlers:
            logger.setLevel(logging.INFO)
            handler = logging.StreamHandler()
            handler.setFormatter(
                logging.Formatter(
                    "%(asctime)s - %(levelname)s - %(name)s - %(message)s"
                )
            )
            logger.addHandler(handler)
        logger.propagate = False
        return logger
    # ------------------------------------------------------------------ #
    #                   –£—Ç–∏–ª–∏—Ç–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ —Å—Å—ã–ª–æ–∫                       #
    # ------------------------------------------------------------------ #
    @staticmethod
    def normalize_links(raw_links: List[str]) -> List[str]:
        """
        –û—á–∏—Å—Ç–∫–∞ —Å—Å—ã–ª–æ–∫: —É–¥–∞–ª–µ–Ω–∏–µ –ø—É—Å—Ç—ã—Ö —Å—Ç—Ä–æ–∫/–ø—Ä–æ–±–µ–ª–æ–≤, –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ http/https,
        —É–¥–∞–ª–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–∞—é—â–µ–≥–æ —Å–ª–µ—à–∞, —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ c —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –ø–æ—Ä—è–¥–∫–∞.
        """
        cleaned: List[str] = []
        seen: set[str] = set()
        for item in raw_links:
            link = item.strip()
            if not link:
                continue
            if not re.match(r"^https?://", link, flags=re.IGNORECASE):
                link = "http://" + link
            link = link.rstrip("/")
            if link not in seen:
                cleaned.append(link)
                seen.add(link)
        return cleaned
    # ------------------------------------------------------------------ #
    #            –í–∞–ª–∏–¥–∞—Ü–∏—è URL + –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ items_per_page=3000           #
    # ------------------------------------------------------------------ #
    def _validate_links(self) -> None:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å URL –∏ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫–∞–∂–¥—É—é —Å—Å—ã–ª–∫—É –∫ –≤–∏–¥—É,
        –≥–¥–µ –ø–∞—Ä–∞–º–µ—Ç—Ä `items_per_page` –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ —Ä–∞–≤–µ–Ω 3000.
        """
        if not self.links:
            raise ValueError(
                "–°–ø–∏—Å–æ–∫ —Å—Å—ã–ª–æ–∫ –ø—É—Å—Ç –∏–ª–∏ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–æ–ª—å–∫–æ –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã."
            )

        url_re = re.compile(r"^https?://[\w\-.:/?#=&%~+]+$", re.IGNORECASE)
        invalid: List[str] = []
        processed: List[str] = []

        for url in self.links:
            if not url_re.match(url):
                invalid.append(url)
                continue

            parsed = urlparse(url)
            query_dict = dict(parse_qsl(parsed.query, keep_blank_values=True))
            query_dict["items_per_page"] = "3000"  # –≤—Å–µ–≥–¥–∞ 3000

            new_query = urlencode(query_dict, doseq=True)
            new_url = urlunparse(parsed._replace(query=new_query))
            processed.append(new_url)

        if invalid:
            raise ValueError("–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ URL: " + ", ".join(invalid))

        self.links = processed  # —Å–æ—Ö—Ä–∞–Ω—è–µ–º ¬´–¥–æ—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ¬ª —Å—Å—ã–ª–∫–∏
    # ------------------------------------------------------------------ #
    #                        PRIVATE HELPERS                              #
    # ------------------------------------------------------------------ #
    @staticmethod
    def _clean_text(text: str) -> str:
        """–£–¥–∞–ª—è–µ—Ç –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã, \xa0 –∏ &nbsp;."""
        return " ".join(text.replace("\xa0", " ").replace("&nbsp;", " ").split()).strip()

    @staticmethod
    def _clean_price(text: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ü–∏—Ñ—Ä—ã –∏ –¥–µ—Å—è—Ç–∏—á–Ω—ã–µ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏, —É–±–∏—Ä–∞—è –ø—Ä–æ–±–µ–ª—ã –∏ –≤–∞–ª—é—Ç—É."""
        no_nbsp = text.replace("\xa0", " ").replace("&nbsp;", " ")
        return re.sub(r"[^0-9.,]", "", no_nbsp).replace(" ", "")
    # ------------------------------------------------------------------ #
    #               –ó–∞–≥–æ–ª–æ–≤–æ–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ ‚Üí –∏–º—è –ª–∏—Å—Ç–∞ Excel                #
    # ------------------------------------------------------------------ #
    def _extract_page_title(self, soup: BeautifulSoup) -> str:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (—Ç–µ–∫—Å—Ç <h1>)."""
        tag = soup.select_one("div.ty-mainbox-container h1.ty-mainbox-title span")
        if not tag:
            tag = soup.find("h1")
        return self._clean_text(tag.get_text()) if tag else "–ö–∞—Ç–µ–≥–æ—Ä–∏—è"

    def _make_unique_sheet_name(self, title: str) -> str:
        """–°–æ–∑–¥–∞—ë—Ç —É–Ω–∏–∫–∞–ª—å–Ω–æ–µ –∏–º—è –ª–∏—Å—Ç–∞, —É—á–∏—Ç—ã–≤–∞—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è Excel (‚â§31‚ÄØ—Å–∏–º–≤–æ–ª)."""
        # —É–±–∏—Ä–∞–µ–º –∑–∞–ø—Ä–µ—â—ë–Ω–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
        safe = re.sub(r"[:\\/?*\[\]]", " ", title).strip()
        if not safe:
            safe = "Sheet"

        base = safe[:31]  # –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±—Ä–µ–∑–∞–Ω–∏–µ –¥–æ –ª–∏–º–∏—Ç–∞
        count = self._sheet_name_counts.get(base, 0)

        if count:
            # –µ—Å–ª–∏ –∏–º—è —É–∂–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–æ—Å—å, –¥–æ–±–∞–≤–ª—è–µ–º —Å—É—Ñ—Ñ–∏–∫—Å _n
            while True:
                count += 1
                suffix = f"_{count}"
                candidate = (base[: 31 - len(suffix)]) + suffix
                if candidate not in self._sheet_name_counts:
                    safe = candidate
                    break
        else:
            safe = base

        self._sheet_name_counts[safe] = 1
        return safe
    # ------------------------------------------------------------------ #
    #                         EXTRACTORS (v1)                            #
    # ------------------------------------------------------------------ #
    def _extract_row_data_v1(self, row: Tag) -> Dict[str, str] | None:
        """–¢–∞–±–ª–∏—á–Ω–∞—è –≤–µ—Ä—Å—Ç–∫–∞ (v1)."""
        name_td = row.find("td", class_="gr-title hidden_mobile")
        if not name_td or not name_td.a:
            return None
        name = self._clean_text(name_td.a.get_text())

        brand_td = name_td.find_next_sibling("td")
        brand = self._clean_text(brand_td.get_text()) if brand_td else "–ù/–î"

        article_span = row.find("span", onclick=re.compile(r"copyCode\('\d+"))
        article = (
            f"119-{self._clean_text(article_span.get_text())}" if article_span else "–ù/–î"
        )

        price_span = row.find("span", class_="ty-price-num")
        price = self._clean_price(price_span.get_text()) if price_span else "–ù/–î"

        avail_span = row.select_one("div.rg-available > span")
        availability = self._clean_text(avail_span.get_text()) if avail_span else "–ù/–î"

        return {
            "–ù–∞–∑–≤–∞–Ω–∏–µ": name,
            "–ë—Ä–µ–Ω–¥": brand,
            "–ê—Ä—Ç–∏–∫—É–ª": article,
            "–¶–µ–Ω–∞": price,
            "–ù–∞–ª–∏—á–∏–µ": availability,
        }
    # ------------------------------------------------------------------ #
    #                         EXTRACTORS (v2)                            #
    # ------------------------------------------------------------------ #
    def _extract_row_data_v2(self, name_div: Tag) -> Dict[str, str] | None:
        """–ë–ª–æ—á–Ω–∞—è –≤–µ—Ä—Å—Ç–∫–∞ (v2). –ü—Ä–∏–Ω–∏–º–∞–µ—Ç <div class="name_value_pc">."""
        if not name_div or not name_div.a:
            return None
        name = self._clean_text(name_div.a.get_text())

        brand_block = name_div.find_next("div", class_="brand_list compact pc")
        brand = "–ù/–î"
        article = "–ù/–î"
        if brand_block:
            brand_link = brand_block.select_one("div.ty-features-list a")
            if brand_link:
                brand = self._clean_text(brand_link.get_text())
            span_article = brand_block.find("span", style=re.compile("cursor"))
            if span_article:
                article = f"119-{self._clean_text(span_article.get_text())}"

        price_span = name_div.find_next("span", class_="ty-price-num")
        price = self._clean_price(price_span.get_text()) if price_span else "–ù/–î"

        avail_p = name_div.find_next("p", style=re.compile(r"margin:5px"))
        availability = "–ù/–î"
        if avail_p:
            avail_span = avail_p.find("span")
            if avail_span:
                availability = self._clean_text(avail_span.get_text())

        return {
            "–ù–∞–∑–≤–∞–Ω–∏–µ": name,
            "–ë—Ä–µ–Ω–¥": brand,
            "–ê—Ä—Ç–∏–∫—É–ª": article,
            "–¶–µ–Ω–∞": price,
            "–ù–∞–ª–∏—á–∏–µ": availability,
        }
    # ------------------------------------------------------------------ #
    #                 –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Ä—Å–∏–∏ + –ø–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã              #
    # ------------------------------------------------------------------ #
    def _parse_category_page(self, soup: BeautifulSoup) -> List[Dict[str, str]]:
        """–ü—ã—Ç–∞–µ—Ç—Å—è —Å–Ω–∞—á–∞–ª–∞ v1, –∑–∞—Ç–µ–º v2 (–µ—Å–ª–∏ v1 –Ω–µ –Ω–∞–π–¥–µ–Ω–∞)."""
        products: List[Dict[str, str]] = []

        # --- v1 -------------------------------------------------------- #
        rows = soup.select("tr")
        for row in rows:
            data = self._extract_row_data_v1(row)
            if data:
                products.append(data)
        if products:
            return products

        # --- v2 -------------------------------------------------------- #
        for name_div in soup.select("div.name_value_pc"):
            data = self._extract_row_data_v2(name_div)
            if data:
                products.append(data)
        return products
    # ------------------------------------------------------------------ #
    #                      –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ run()                          #
    # ------------------------------------------------------------------ #
    def run(self) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:
        """
        –û–±—Ö–æ–¥–∏—Ç –≤—Å–µ —Å—Å—ã–ª–∫–∏, –∞–∫–∫—É–º—É–ª–∏—Ä—É—è —Ç–æ–≤–∞—Ä—ã –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É.
        (–ú–µ—Ç–æ–¥ —Å–æ—Ö—Ä–∞–Ω—ë–Ω –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏; —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –ª–∏—Å—Ç–æ–≤
         —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç—Å—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –≤ self._sheet_data.)
        """
        all_products: List[Dict[str, Any]] = []
        failed_links: List[str] = []
        success_pages = 0

        for url in self.links:
            self.logger.info("–ó–∞–≥—Ä—É–∂–∞–µ–º: %s", url)
            soup = self.parser.get_page(url)
            if not soup:
                self.logger.warning("–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: %s", url)
                failed_links.append(url)
                continue

            products = self._parse_category_page(soup)
            self.logger.info("\u2514‚Äî —Ç–æ–≤–∞—Ä–æ–≤: %d", len(products))
            all_products.extend(products)
            success_pages += 1

            # ----------------  –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ª–∏—Å—Ç–∞ ----------------------- #
            title = self._extract_page_title(soup)
            sheet_name = self._make_unique_sheet_name(title)
            self._sheet_data[sheet_name] = products

        stats = {
            "total": len(self.links),
            "success": success_pages,
            "failed": len(failed_links),
            "failed_links": failed_links,
            "total_products": len(all_products),
        }
        self.logger.info(
            "–ò—Ç–æ–≥–æ | –∫–∞—Ç–µ–≥–æ—Ä–∏–π: %(total)d | —É—Å–ø–µ—Ö: %(success)d "
            "| –æ—à–∏–±–æ–∫: %(failed)d | —Ç–æ–≤–∞—Ä–æ–≤: %(total_products)d",
            stats,
        )
        return all_products, stats
    # ------------------------------------------------------------------ #
    #                   –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –≤ Excel                    #
    # ------------------------------------------------------------------ #
    def save_results(self) -> bytes:
        """
        –ó–∞–ø–∏—Å—ã–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ Excel‚Äë—Ñ–∞–π–ª, —Å–æ–∑–¥–∞–≤–∞—è –æ—Ç–¥–µ–ª—å–Ω—ã–π –ª–∏—Å—Ç
        –¥–ª—è –∫–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –±–∏–Ω–∞—Ä–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è.
        """
        if not self._sheet_data:
            raise RuntimeError("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è. –°–Ω–∞—á–∞–ª–∞ –≤—ã–∑–æ–≤–∏—Ç–µ run().")

        self.logger.info("–°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ %s", self.output_file)
        buffer = BytesIO()

        with pd.ExcelWriter(buffer, engine="xlsxwriter") as writer:
            for sheet_name, rows in self._sheet_data.items():
                df = pd.DataFrame(rows)
                # –ª–∏—Å—Ç—ã Excel –Ω–µ¬†–¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –ø—É—Å—Ç—ã–º–∏¬†‚Äî –ø—Ä–æ–≤–µ—Ä—è–µ–º
                if df.empty:
                    df = pd.DataFrame({"–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö": []})
                df.to_excel(writer, sheet_name=sheet_name[:31], index=False)
        buffer.seek(0)
        # —Å–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–∞ –¥–∏—Å–∫
        Path(self.output_file).write_bytes(buffer.getvalue())
        self.logger.info(
            "–§–∞–π–ª %s —Å–æ–∑–¥–∞–Ω (%d¬†–ª–∏—Å—Ç–æ–≤)",
            Path(self.output_file).name,
            len(self._sheet_data),
        )
        buffer.seek(0)
        return buffer.getvalue()
```

---